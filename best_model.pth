{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11485961,"sourceType":"datasetVersion","datasetId":7199153}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Block 1: Install Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -q \"numpy<2.0\" opencv-python-headless==4.8.1.78 timm albumentations PyYAML einops","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 2: Create Project Directories","metadata":{}},{"cell_type":"code","source":"import os\n\nPROJECT_ROOT = '/kaggle/working/camoXpert'\nDIRS = [\n    'configs',\n    'data',\n    'losses',\n    'metrics',\n    'models',\n    'scripts',\n    'results/checkpoints'\n]\n\n# Create all directories\nfor d in DIRS:\n    os.makedirs(os.path.join(PROJECT_ROOT, d), exist_ok=True)\n\n# Create __init__.py files to make them packages\nINIT_DIRS = ['data', 'losses', 'metrics', 'models']\nfor d in INIT_DIRS:\n    with open(os.path.join(PROJECT_ROOT, d, '__init__.py'), 'w') as f:\n        pass\n\nprint(\"Project directories created successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 3: Write Config Files","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/configs/config.yaml\n# --- General Config ---\nseed: 42\n\n# --- Device Config ---\ndevice:\n  num_workers: 2\n  pin_memory: True\n  mixed_precision: True\n\n# --- Dataset Config ---\ndataset:\n  num_classes: 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/configs/hyperparameters.yaml\n# --- Hyperparameters ---\noptimizer:\n  learning_rate: 1.0e-4\n  weight_decay: 0.01\n\nscheduler:\n  type: 'CosineAnnealingWarmRestarts'\n  t_0: 10\n  t_mult: 2\n\nloss_weights:\n  bce: 1.0\n  dice: 1.0\n  structure: 0.5\n  aux: 0.1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 4: Write Data Pipeline Files","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/data/augmentations.py\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nclass AdvancedCODAugmentation:\n    def __init__(self, img_size=352, is_train=True):\n        self.img_size = img_size\n        self.is_train = is_train\n\n        if self.is_train:\n            self.transform = A.Compose([\n                A.Resize(img_size, img_size),\n                A.HorizontalFlip(p=0.5),\n                A.VerticalFlip(p=0.5),\n                A.RandomRotate90(p=0.5),\n                A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.7),\n                A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n                A.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225],\n                ),\n                ToTensorV2(),\n            ], p=1.0, is_check_shapes=False) # <-- FIX ADDED\n        else:\n            self.transform = A.Compose([\n                A.Resize(img_size, img_size),\n                A.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225],\n                ),\n                ToTensorV2(),\n            ], p=1.0, is_check_shapes=False) # <-- FIX ADDED\n\n    def __call__(self, image, mask):\n        return self.transform(image=image, mask=mask)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/data/dataset.py\nimport os\nimport cv2\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom .augmentations import AdvancedCODAugmentation\n\nclass CODDataset(Dataset):\n    def __init__(self, root_dir, dataset_name='COD10K', split='train', img_size=352, augment=True, debug_len=None):\n        self.root_dir = root_dir\n        self.dataset_name = dataset_name\n        self.split = split\n        self.img_size = img_size\n        self.augment = augment and split == 'train'\n\n        print(f\"Loading dataset: {dataset_name} ({split})\")\n        \n        if split == 'train':\n            self.image_dir = os.path.join(root_dir, 'Train', 'Image')\n            self.mask_dir = os.path.join(root_dir, 'Train', 'GT_Object')\n        else:\n            self.image_dir = os.path.join(root_dir, 'Test', 'Image')\n            self.mask_dir = os.path.join(root_dir, 'Test', 'GT_Object')\n       \n        if not os.path.isdir(self.image_dir):\n            raise FileNotFoundError(f\"Image directory not found: {self.image_dir}\")\n\n        self.image_list = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n        \n        if debug_len is not None:\n            self.image_list = self.image_list[:debug_len]\n\n        self.transform = AdvancedCODAugmentation(img_size=img_size, is_train=self.augment)\n        print(f\"Found {len(self.image_list)} images in {self.image_dir}\")\n\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, idx):\n        img_name = self.image_list[idx]\n        img_path = os.path.join(self.image_dir, img_name)\n\n        image = cv2.imread(img_path)\n        if image is None:\n            raise IOError(f\"Could not read image: {img_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        mask_extensions = ['.png', '.jpg', '.jpeg', '.PNG', '.JPG']\n        base_name = os.path.splitext(img_name)[0]\n\n        mask = None\n        for ext in mask_extensions:\n            mask_name_try = base_name + ext\n            if os.path.exists(os.path.join(self.mask_dir, mask_name_try)):\n                mask_path = os.path.join(self.mask_dir, mask_name_try)\n                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n                break\n        \n        if mask is None:\n             mask_path = os.path.join(self.mask_dir, img_name)\n             if os.path.exists(mask_path):\n                 mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n             else:\n                raise ValueError(f\"Failed to load mask for: {img_name} at {self.mask_dir}\")\n        \n        if mask is None:\n            raise IOError(f\"Could not read mask for image: {img_name}\")\n\n        mask = (mask > 128).astype(np.float32)\n        transformed = self.transform(image=image, mask=mask)\n        image_tensor = transformed['image']\n        mask_tensor = transformed['mask']\n\n        return image_tensor, mask_tensor.unsqueeze(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 5: Write Loss Functions","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/losses/dice_loss.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-5):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n    def forward(self, pred, target):\n        pred_sig = torch.sigmoid(pred)\n        intersection = (pred_sig * target).sum(dim=(2, 3))\n        union = pred_sig.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n        return 1.0 - dice.mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/losses/structure_loss.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass StructureLoss(nn.Module):\n    def __init__(self):\n        super(StructureLoss, self).__init__()\n        self.criterion = nn.L1Loss()\n    def forward(self, pred, target):\n        pred_sig = torch.sigmoid(pred)\n        target = target.float()\n        \n        # Sobel operator for edge detection\n        weight_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)\n        weight_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)\n        \n        pred_edge_x = F.conv2d(pred_sig, weight_x, padding=1)\n        pred_edge_y = F.conv2d(pred_sig, weight_y, padding=1)\n        pred_edge = torch.abs(pred_edge_x) + torch.abs(pred_edge_y)\n        \n        target_edge_x = F.conv2d(target, weight_x, padding=1)\n        target_edge_y = F.conv2d(target, weight_y, padding=1)\n        target_edge = torch.abs(target_edge_x) + torch.abs(target_edge_y)\n        \n        return self.criterion(pred_edge, target_edge)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/losses/advanced_loss.py\nimport torch\nimport torch.nn as nn\nfrom .dice_loss import DiceLoss\nfrom .structure_loss import StructureLoss\n\nclass AdvancedCODLoss(nn.Module):\n    def __init__(self, bce_weight=1.0, iou_weight=1.0, edge_weight=0.5, aux_weight=0.1):\n        super(AdvancedCODLoss, self).__init__()\n        self.bce = nn.BCEWithLogitsLoss()\n        self.dice = DiceLoss()\n        self.edge = StructureLoss()\n        self.bce_weight = bce_weight\n        self.iou_weight = iou_weight\n        self.edge_weight = edge_weight\n        self.aux_weight = aux_weight\n\n    def forward(self, pred, target, aux_loss=None, deep_outputs=None):\n        bce = self.bce(pred, target)\n        iou = self.dice(pred, target)\n        edge = self.edge(pred, target)\n        total_loss = (self.bce_weight * bce + self.iou_weight * iou + self.edge_weight * edge)\n        loss_dict = {'bce': bce.item(), 'iou': iou.item(), 'edge': edge.item()}\n\n        # This will be a DataParallel-gathered tensor [N]\n        if aux_loss is not None:\n            if aux_loss.dim() > 0:\n                aux_loss = aux_loss.mean()\n            total_loss += self.aux_weight * aux_loss\n            loss_dict['aux'] = aux_loss.item()\n\n        if deep_outputs is not None:\n            deep_loss = 0\n            for i, deep_pred in enumerate(deep_outputs):\n                deep_loss += self.bce(deep_pred, target)\n            deep_loss /= len(deep_outputs)\n            total_loss += 0.4 * deep_loss\n            loss_dict['deep'] = deep_loss.item()\n        \n        loss_dict['total'] = total_loss.item()\n        return total_loss, loss_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 6: Write Metrics File","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/metrics/cod_metrics.py\nimport torch\n\nclass CODMetrics:\n    def __init__(self, device, smooth=1e-5):\n        self.device = device\n        self.smooth = smooth\n\n    def compute_all(self, pred, target):\n        pred_sig = torch.sigmoid(pred)\n        pred_bin = (pred_sig > 0.5).float()\n        target = target.float()\n        \n        intersection = (pred_bin * target).sum()\n        union = pred_bin.sum() + target.sum()\n        \n        # IoU (Jaccard)\n        iou = (intersection + self.smooth) / (union - intersection + self.smooth)\n        \n        # Dice\n        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n        \n        return {\n            'IoU': iou.item(),\n            'Dice_Score': dice.item()\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 7: Write Model Utilities","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/models/utils.py\nimport torch\nimport random\nimport numpy as np\nimport yaml\nimport os\n\nclass AverageMeter:\n    def __init__(self):\n        self.reset()\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef count_parameters(model):\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return total_params, trainable_params\n\ndef load_config(path):\n    with open(path, 'r') as f:\n        return yaml.safe_load(f)\n\ndef save_checkpoint(state, save_dir, filename='checkpoint.pth'):\n    os.makedirs(save_dir, exist_ok=True)\n    torch.save(state, os.path.join(save_dir, filename))\n    print(f\"\\n‚úì Checkpoint saved at {os.path.join(save_dir, filename)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/models/fusion.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .backbone import LayerNorm2d\nfrom einops import rearrange\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size=3, padding=1, stride=1, dilation=1):\n        super().__init__()\n        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size, stride, padding, dilation=dilation, bias=False)\n        self.norm = LayerNorm2d(out_dim)\n        self.act = nn.GELU()\n    def forward(self, x):\n        return self.act(self.norm(self.conv(x)))\n\nclass CrossAttention(nn.Module):\n    def __init__(self, in_dim, num_heads=8, qkv_bias=True, scale=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = in_dim // num_heads\n        self.scale = scale or head_dim ** -0.5\n        self.q_conv = nn.Conv2d(in_dim, in_dim, 1, bias=qkv_bias)\n        self.k_conv = nn.Conv2d(in_dim, in_dim, 1, bias=qkv_bias)\n        self.v_conv = nn.Conv2d(in_dim, in_dim, 1, bias=qkv_bias)\n        self.proj = nn.Conv2d(in_dim, in_dim, 1)\n\n    def forward(self, x, y):\n        B, C, H, W = x.shape\n        q = self.q_conv(x).flatten(2).transpose(1, 2)\n        k = self.k_conv(y).flatten(2).transpose(1, 2)\n        v = self.v_conv(y).flatten(2).transpose(1, 2)\n        \n        q = rearrange(q, 'b n (h d) -> b h n d', h=self.num_heads)\n        k = rearrange(k, 'b n (h d) -> b h n d', h=self.num_heads)\n        v = rearrange(v, 'b n (h d) -> b h n d', h=self.num_heads)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        \n        out = (attn @ v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out = out.transpose(1, 2).reshape(B, C, H, W)\n        return self.proj(out) + x\n\nclass FeatureFusion(nn.Module):\n    def __init__(self, dim, num_heads=8):\n        super().__init__()\n        self.cross_attn = CrossAttention(dim, num_heads)\n        self.conv_block = ConvBlock(dim, dim)\n    def forward(self, high_feat, low_feat):\n        low_feat_upsampled = F.interpolate(low_feat, size=high_feat.shape[2:], mode='bilinear', align_corners=True)\n        fused_feat = self.cross_attn(high_feat, low_feat_upsampled)\n        return self.conv_block(fused_feat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/models/segmentation_head.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .backbone import LayerNorm2d\nfrom einops import rearrange\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size=3, padding=1, stride=1, dilation=1):\n        super().__init__()\n        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size, stride, padding, dilation=dilation, bias=False)\n        self.norm = LayerNorm2d(out_dim)\n        self.act = nn.GELU()\n    def forward(self, x):\n        return self.act(self.norm(self.conv(x)))\n\nclass Decoder(nn.Module):\n    def __init__(self, encoder_dims, decoder_dims):\n        super().__init__()\n        self.conv_blocks = nn.ModuleList()\n        for i in range(len(decoder_dims)):\n            if i == 0:\n                in_c = encoder_dims[-1]\n            else:\n                in_c = decoder_dims[i-1] + encoder_dims[-(i+1)]\n            out_c = decoder_dims[i]\n            self.conv_blocks.append(ConvBlock(in_c, out_c))\n            \n    def forward(self, features):\n        x = self.conv_blocks[0](features[-1])\n        decoded_features = [x]\n        \n        for i in range(1, len(self.conv_blocks)):\n            x = F.interpolate(x, size=features[-(i+1)].shape[2:], mode='bilinear', align_corners=True)\n            x = torch.cat([x, features[-(i+1)]], dim=1)\n            x = self.conv_blocks[i](x)\n            decoded_features.append(x)\n            \n        return decoded_features\n\nclass SegmentationHead(nn.Module):\n    def __init__(self, in_dim, mid_dim, out_dim, num_classes, deep_supervision=True):\n        super().__init__()\n        self.deep_supervision = deep_supervision\n        self.conv_in = ConvBlock(in_dim, mid_dim)\n        self.conv_out = nn.Conv2d(mid_dim, num_classes, 1)\n        if self.deep_supervision:\n            self.deep_head_3 = nn.Conv2d(out_dim[1], num_classes, 1)\n            self.deep_head_2 = nn.Conv2d(out_dim[2], num_classes, 1)\n\n    def forward(self, features, deep_features=None):\n        x = self.conv_in(features)\n        x = self.conv_out(x)\n        \n        if self.training and self.deep_supervision and deep_features is not None:\n            out_3 = self.deep_head_3(deep_features[1])\n            out_2 = self.deep_head_2(deep_features[2])\n            return x, [out_3, out_2]\n            \n        return x, []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 8: Write Model Backbones (SDTA & Experts)","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/models/backbone.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.models.layers import DropPath, trunc_normal_\nfrom einops import rearrange\n\nclass LayerNorm2d(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim, eps=1e-6)\n    def forward(self, x):\n        x = rearrange(x, 'b c h w -> b h w c').contiguous()\n        x = self.norm(x)\n        return rearrange(x, 'b h w c -> b c h w').contiguous()\n\nclass SDTAEncoder(nn.Module):\n    def __init__(self, in_dim, num_heads=8, qkv_bias=True, scale=None, drop_path=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = in_dim // num_heads\n        self.scale = scale or head_dim ** -0.5\n\n        self.q_conv = nn.Conv2d(in_dim, in_dim, 1, bias=qkv_bias)\n        self.k_conv = nn.Conv2d(in_dim, in_dim, 1, bias=qkv_bias)\n        self.v_conv = nn.Conv2d(in_dim, in_dim, 1, bias=qkv_bias)\n        self.proj = nn.Conv2d(in_dim, in_dim, 1)\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        q = self.q_conv(x).flatten(2).transpose(1, 2)\n        k = self.k_conv(x).flatten(2).transpose(1, 2)\n        v = self.v_conv(x).flatten(2).transpose(1, 2)\n        \n        q = rearrange(q, 'b n (h d) -> b h n d', h=self.num_heads)\n        k = rearrange(k, 'b n (h d) -> b h n d', h=self.num_heads)\n        v = rearrange(v, 'b n (h d) -> b h n d', h=self.num_heads)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        \n        out = (attn @ v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out = out.transpose(1, 2).reshape(B, C, H, W)\n        return x + self.drop_path(self.proj(out))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/models/experts.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .backbone import LayerNorm2d\nfrom einops import rearrange\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_dim, out_dim, kernel_size=3, padding=1, stride=1, dilation=1):\n        super().__init__()\n        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size, stride, padding, dilation=dilation, bias=False)\n        self.norm = LayerNorm2d(out_dim)\n        self.act = nn.GELU()\n    def forward(self, x):\n        return self.act(self.norm(self.conv(x)))\n\nclass TextureExpert(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.branch1 = ConvBlock(dim, dim // 4, kernel_size=1, padding=0)\n        self.branch2 = ConvBlock(dim, dim // 4, kernel_size=3, padding=2, dilation=2)\n        self.branch3 = ConvBlock(dim, dim // 4, kernel_size=3, padding=4, dilation=4)\n        self.branch4 = ConvBlock(dim, dim // 4, kernel_size=3, padding=6, dilation=6)\n        self.out_conv = ConvBlock(dim, dim, kernel_size=1, padding=0)\n    def forward(self, x):\n        b1, b2, b3, b4 = self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)\n        out = torch.cat([b1, b2, b3, b4], dim=1)\n        return self.out_conv(out)\n\nclass BoundaryExpert(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.conv_a = ConvBlock(dim, dim, kernel_size=3)\n        self.conv_b = ConvBlock(dim, dim, kernel_size=3)\n    def forward(self, x):\n        return self.conv_b(self.conv_a(x))\n\nclass ContrastExpert(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Linear(dim, dim // 4)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(dim // 4, dim)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        b, c, _, _ = x.shape\n        pooled_features = self.pool(x).view(b, c)\n        scores = self.sigmoid(self.fc2(self.act(self.fc1(pooled_features))))\n        return x * scores.view(b, c, 1, 1)\n\nclass MoERouter(nn.Module):\n    def __init__(self, in_dim, num_experts, top_k=2):\n        super().__init__()\n        self.num_experts = num_experts\n        self.top_k = top_k\n        self.gate = nn.Linear(in_dim, num_experts)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x_flat = x.mean(dim=(2, 3))\n        logits = self.gate(x_flat)\n        weights, indices = torch.topk(logits, self.top_k, dim=1)\n        weights = self.softmax(weights)\n        \n        logits_soft = self.softmax(logits)\n        expert_counts = logits_soft.sum(dim=0)\n        aux_loss = (expert_counts * (expert_counts.log() + 1e-8)).mean()\n        \n        return weights, indices, aux_loss\n\nclass MoELayer(nn.Module):\n    def __init__(self, in_dim, num_experts=3, top_k=2):\n        super().__init__()\n        self.router = MoERouter(in_dim, num_experts, top_k)\n        self.top_k = top_k\n        self.num_experts = num_experts\n        \n        self.experts = nn.ModuleList([\n            TextureExpert(in_dim),\n            BoundaryExpert(in_dim),\n            ContrastExpert(in_dim)\n        ])\n        \n        assert len(self.experts) == self.num_experts, \"Expert count mismatch\"\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        weights, indices, aux_loss = self.router(x)\n        output = torch.zeros_like(x)\n        for i in range(b):\n            top_indices, top_weights = indices[i], weights[i]\n            for j in range(self.top_k):\n                # --- This .item() call is the one that ---\n                # --- caused our multi-GPU crash, but ---\n                # --- is 100% safe in our new validate() ---\n                expert_idx = top_indices[j].item()\n                expert_weight = top_weights[j]\n                output[i] += expert_weight * self.experts[expert_idx](x[i].unsqueeze(0)).squeeze(0)\n        return output, aux_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 9: Write the Main Model","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/models/camoxpert.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.models import create_model\nfrom .experts import MoELayer\nfrom .fusion import FeatureFusion\nfrom .segmentation_head import Decoder, SegmentationHead\nfrom .sinet_encoder import SINetEncoder # <-- IMPORTING RES2NET\n\n# --- This is now a HYBRID model registry ---\nBACKBONE_REGISTRY = {\n    'edgenext_small': 'edgenext_small',\n    'res2net_ncd': SINetEncoder  # <-- ADDED RES2NET\n}\n\nclass CamoXpert(nn.Module):\n    def __init__(self, num_classes=1, backbone='edgenext_small', num_experts=3, top_k=2, pretrained=True):\n        super(CamoXpert, self).__init__()\n        \n        print(\"\\n\" + \"=\"*70)\n        print(f\"INITIALIZING CAMOXPERT (OPTIMIZED HYBRID - NO SDTA)\")\n        print(\"=\"*70)\n        print(f\"Backbone: {backbone}, Pretrained: {pretrained}, Experts: {num_experts}\")\n\n        self.backbone_name = backbone\n        self.deep_supervision = True\n\n        # --- 1. Load the Backbone ---\n        if backbone == 'res2net_ncd':\n            self.backbone = SINetEncoder(pretrained=pretrained)\n            encoder_dims = self.backbone.channels()\n        # --- THIS IS THE FIX: Check the registry before raising an error ---\n        elif backbone in BACKBONE_REGISTRY:\n            self.backbone = create_model(\n                BACKBONE_REGISTRY[backbone],\n                pretrained=pretrained,\n                features_only=True\n            )\n            encoder_dims = self.backbone.feature_info.channels()\n        else:\n            raise NotImplementedError(f\"Backbone {backbone} not supported.\")\n        \n        print(f\"Encoder Dims: {encoder_dims}\")\n        \n        # --- 2. SDTA Encoders (REMOVED) ---\n        \n        # --- 3. Mixture of Experts (MoE) Layers ---\n        self.moe_layers = nn.ModuleList([\n            MoELayer(in_dim=dim, num_experts=num_experts, top_k=top_k) for dim in encoder_dims\n        ])\n        \n        # --- 4. Decoder ---\n        decoder_dims = [256, 128, 64, 32]\n        self.decoder = Decoder(encoder_dims, decoder_dims)\n        \n        # --- 5. Segmentation Head ---\n        self.seg_head = SegmentationHead(\n            in_dim=decoder_dims[-1],\n            mid_dim=decoder_dims[-1] // 2,\n            out_dim=decoder_dims,\n            num_classes=num_classes,\n            deep_supervision=self.deep_supervision\n        )\n\n    def forward(self, x, return_deep_supervision=True):\n        # --- Encoder Path ---\n        f1, f2, f3, f4 = self.backbone(x)\n        \n        # --- MoE Layers now take backbone features directly ---\n        f1_moe, aux1 = self.moe_layers[0](f1)\n        f2_moe, aux2 = self.moe_layers[1](f2)\n        f3_moe, aux3 = self.moe_layers[2](f3)\n        f4_moe, aux4 = self.moe_layers[3](f4)\n        \n        aux_loss = (aux1 + aux2 + aux3 + aux4) / 4.0\n        features = [f1_moe, f2_moe, f3_moe, f4_moe]\n        \n        # --- Decoder Path ---\n        decoded_features = self.decoder(features)\n        \n        # --- Output Head ---\n        final_mask, deep_masks = self.seg_head(\n            decoded_features[-1], \n            decoded_features\n        )\n        \n        final_mask = F.interpolate(final_mask, size=x.shape[2:], mode='bilinear', align_corners=True)\n        \n        if self.training and return_deep_supervision:\n            deep_masks = [F.interpolate(dm, size=x.shape[2:], mode='bilinear', align_corners=True) for dm in deep_masks]\n            return final_mask, aux_loss, deep_masks\n        \n        return final_mask, aux_loss, []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 10 (Correction): Fix OOM (Batch Size 4)","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/scripts/main.py\n\"\"\"\nCamoXpert - Main training script\n(Kaggle, Multi-GPU, Resume-Ready)\n(Running the ACCURATE Res2Net model for 3 EPOCHS)\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nimport json\nfrom tqdm import tqdm\nimport yaml # Added import for config loading\n\nPROJECT_ROOT = '/kaggle/working' \nsys.path.insert(0, PROJECT_ROOT)\n\nfrom camoXpert.models.camoxpert import CamoXpert\nfrom camoXpert.models.utils import count_parameters, set_seed, load_config, save_checkpoint, AverageMeter\nfrom camoXpert.data.dataset import CODDataset\nfrom camoXpert.losses.advanced_loss import AdvancedCODLoss\nfrom camoXpert.metrics.cod_metrics import CODMetrics\n\ndef train_one_epoch(model, dataloader, criterion, optimizer, scaler, device, deep_supervision=True):\n    model.train()\n    loss_meter = AverageMeter()\n    \n    for images, masks in tqdm(dataloader, desc=\"Training\", ncols=100):\n        images, masks = images.to(device), masks.to(device)\n        \n        with torch.amp.autocast('cuda', enabled=True):\n            pred, aux_loss, deep_outputs = model(images, return_deep_supervision=deep_supervision)\n            loss, loss_dict = criterion(pred, masks, aux_loss, deep_outputs)\n            \n            if loss.dim() > 0:\n                loss = loss.mean()\n            \n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n            \n        loss_meter.update(loss.item(), images.size(0))\n\n    return loss_meter.avg\n\n@torch.no_grad()\ndef validate(model, dataloader, metrics, device):\n    is_parallel = isinstance(model, torch.nn.DataParallel)\n    model_to_validate = model.module if is_parallel else model\n    model_to_validate.eval()\n    \n    all_metrics = []\n    \n    for images, masks in tqdm(dataloader, desc=\"Validating\", ncols=100):\n        images, masks = images.to(device), masks.to(device)\n        pred, _, _ = model_to_validate(images, return_deep_supervision=False)\n        batch_metrics = metrics.compute_all(pred, masks)\n        all_metrics.append(batch_metrics)\n        \n    agg_metrics = {key: (sum(d[key] for d in all_metrics) / len(all_metrics)) for key in all_metrics[0]}\n    return agg_metrics\n\ndef run_training(args):\n    CONFIG_PATH = os.path.join(PROJECT_ROOT, 'camoXpert/configs/config.yaml')\n    HYPER_PATH = os.path.join(PROJECT_ROOT, 'camoXpert/configs/hyperparameters.yaml')\n    config = load_config(CONFIG_PATH)\n    hyperparams = load_config(HYPER_PATH)\n    \n    data_path = args.dataset_path\n    chk_dir = args.checkpoint_dir\n    os.makedirs(chk_dir, exist_ok=True)\n    \n    set_seed(config['seed'])\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"--- Using Device: {device} ---\")\n\n    print(f\"--- Loading Data from {data_path} ---\")\n    \n    train_dataset = CODDataset(\n        root_dir=data_path, dataset_name='COD10K', split='train',\n        img_size=args.img_size, augment=True, debug_len=args.debug_train_len\n    )\n    val_dataset = CODDataset(\n        root_dir=data_path, dataset_name='COD10K', split='test',\n        img_size=args.img_size, augment=False, debug_len=args.debug_val_len\n    )\n    \n    train_loader = DataLoader(\n        train_dataset, batch_size=args.batch_size, shuffle=True,\n        num_workers=config['device']['num_workers'], pin_memory=config['device']['pin_memory']\n    )\n    val_loader = DataLoader(\n        val_dataset, batch_size=1, shuffle=False,\n        num_workers=config['device']['num_workers'], pin_memory=config['device']['pin_memory']\n    )\n\n    print(\"--- Loading Model ---\")\n    model = CamoXpert(\n        num_classes=config['dataset']['num_classes'],\n        backbone=args.backbone, num_experts=args.num_experts, pretrained=True\n    ).to(device)\n    \n    if torch.cuda.device_count() > 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs for TRAINING! (DataParallel)\")\n        model = torch.nn.DataParallel(model)\n    \n    total, trainable = count_parameters(model)\n    print(f\"Total Params: {total/1e6:.2f}M | Trainable: {total/1e6:.2f}M\")\n    \n    criterion = AdvancedCODLoss(\n        bce_weight=hyperparams['loss_weights']['bce'],\n        iou_weight=hyperparams['loss_weights']['dice'],\n        edge_weight=hyperparams['loss_weights']['structure']\n    ).to(device)\n    \n    optimizer = AdamW(\n        model.parameters(), lr=hyperparams['optimizer']['learning_rate'],\n        weight_decay=hyperparams['optimizer']['weight_decay']\n    )\n    \n    scheduler = CosineAnnealingWarmRestarts(\n        optimizer, T_0=hyperparams['scheduler']['t_0'],\n        T_mult=hyperparams['scheduler']['t_mult'], eta_min=1e-6\n    )\n    \n    scaler = torch.amp.GradScaler('cuda', enabled=config['device']['mixed_precision'])\n    metrics = CODMetrics(device=device)\n\n    start_epoch = 0\n    best_iou = 0.0\n    # --- Resume logic (will start fresh since chk_dir is new) ---\n    if args.resume_from and os.path.exists(args.resume_from):\n        print(f\"Resuming training from checkpoint: {args.resume_from}\")\n        checkpoint = torch.load(args.resume_from, map_location=device)\n        \n        model_state = checkpoint['model_state_dict']\n        if isinstance(model, torch.nn.DataParallel):\n            model.module.load_state_dict(model_state)\n        else:\n            model.load_state_dict(model_state)\n            \n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        best_iou = checkpoint['best_iou']\n        \n        if 'scheduler_state_dict' in checkpoint:\n            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            print(\"Loaded scheduler state.\")\n            start_epoch = checkpoint['epoch'] + 1\n        else:\n            print(\"Scheduler state not found. Initializing new scheduler.\")\n            start_epoch = checkpoint['epoch'] + 1\n            \n        print(f\"Resumed. Starting from Epoch {start_epoch}, Best IoU: {best_iou:.4f}\")\n\n    print(f\"--- Starting Training from Epoch {start_epoch+1} to {args.epochs} ---\")\n    \n    for epoch in range(start_epoch, args.epochs):\n        print(f\"\\n--- Epoch {epoch+1}/{args.epochs} ---\")\n        \n        train_loss = train_one_epoch(\n            model, train_loader, criterion, optimizer, scaler, device,\n            deep_supervision=args.deep_supervision\n        )\n        val_metrics = validate(model, val_loader, metrics, device)\n        scheduler.step()\n        \n        print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val IoU: {val_metrics['IoU']:.4f} | Val Dice: {val_metrics['Dice_Score']:.4f}\")\n\n        if val_metrics['IoU'] > best_iou:\n            best_iou = val_metrics['IoU']\n            \n            model_state = model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict()\n            \n            save_checkpoint({\n                'epoch': epoch,\n                'model_state_dict': model_state,\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'best_iou': best_iou,\n                'args': vars(args)\n            }, chk_dir, 'best_model.pth')\n            print(f\"  üèÜ NEW BEST! IoU: {best_iou:.4f}\")\n\n    print(f\"\\n{'='*70}\\nTRAINING COMPLETE!\\n{'='*70}\")\n    print(f\"Best IoU:  {best_iou:.4f}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description='CamoXpert (EdgeNeXt) Kaggle Training')\n    parser.add_argument('command', type=str, choices=['train'])\n    parser.add_argument('--dataset-path', type=str, required=True)\n    parser.add_argument('--checkpoint-dir', type=str, default='checkpoints')\n    parser.add_argument('--resume-from', type=str, default=None)\n    \n    parser.add_argument('--backbone', type=str, default='edgenext_small')\n    parser.add_argument('--num-experts', type=int, default=3)\n    parser.add_argument('--batch-size', type=int, default=16)\n    parser.add_argument('--img-size', type=int, default=352)\n    parser.add_argument('--epochs', type=int, default=40)\n    parser.add_argument('--lr', type=float, default=1e-4)\n    parser.add_argument('--deep-supervision', action='store_true')\n    parser.add_argument('--no-deep-supervision', dest='deep_supervision', action='store_false')\n    parser.set_defaults(deep_supervision=True)\n    parser.add_argument('--debug-train-len', type=int, default=None)\n    parser.add_argument('--debug-val-len', type=int, default=None)\n    \n    # --- FAKE ARGS FOR THE (HYBRID Res2Net) KAGGLE RUN ---\n    fake_args = [\n        'train',\n        '--dataset-path', '/kaggle/input/cod10k-dataset/COD10K-v3', \n        # --- Save to a NEW folder ---\n        '--checkpoint-dir', '/kaggle/working/camoXpert/results/resnet_checkpoints',\n        # --- This will fail (file not found) and start a new training run, which is what we want ---\n        '--resume-from', '/kaggle/working/camoXpert/results/resnet_checkpoints/best_model.pth',\n        \n        # --- Using the Res2Net backbone ---\n        '--backbone', 'res2net_ncd', \n        '--num-experts', '3',\n        \n        # --- Using a safe batch size for this large model ---\n        '--batch-size', '8', # (4 images per GPU)\n        \n        '--img-size', '352',\n        '--epochs', '50', # Set to 3 epochs as requested\n        '--deep-supervision'\n    ]\n    \n    print(f\"Running with 'faked' args: {fake_args}\")\n    args = parser.parse_args(args=fake_args)\n\n    if args.command == 'train':\n        run_training(args)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 10.5: Add the Rest2Net model files","metadata":{}},{"cell_type":"code","source":"# Create the 'lib' directory if it doesn't exist\n!mkdir -p /kaggle/working/camoXpert/lib","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/lib/Res2Net_v1b.py\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\n\nmodel_urls = {\n    'res2net50_v1b_26w_4s': 'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_v1b_26w_4s-3cf99910.pth',\n}\n\nclass Bottle2neck(nn.Module):\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, downsample=None, baseWidth=26, scale=4, stype='normal'):\n        super(Bottle2neck, self).__init__()\n        width = int(math.floor(planes * (baseWidth / 64.0)))\n        self.conv1 = nn.Conv2d(inplanes, width * scale, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width * scale)\n        \n        self.nums = scale - 1\n        if stype == 'stage':\n            self.pool = nn.AvgPool2d(3, stride=stride, padding=1)\n        convs, bns = [], []\n        for i in range(self.nums):\n            convs.append(nn.Conv2d(width, width, 3, stride=stride, padding=1, bias=False))\n            bns.append(nn.BatchNorm2d(width))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n\n        self.conv3 = nn.Conv2d(width * scale, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stype = stype\n        self.scale = scale\n        self.width = width\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        for i in range(self.nums):\n            if i == 0 or self.stype == 'stage':\n                sp = spx[i]\n            else:\n                sp = sp + spx[i]\n            sp = self.convs[i](sp)\n            sp = self.relu(self.bns[i](sp))\n            out = sp if i == 0 else torch.cat((out, sp), 1)\n        \n        if self.scale != 1:\n            if self.stype == 'normal':\n                out = torch.cat((out, spx[self.nums]), 1)\n            elif self.stype == 'stage':\n                out = torch.cat((out, self.pool(spx[self.nums])), 1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        out += residual\n        return self.relu(out)\n\nclass Res2Net(nn.Module):\n    def __init__(self, block, layers, baseWidth=26, scale=4, num_classes=1000):\n        self.inplanes = 64\n        super(Res2Net, self).__init__()\n        self.baseWidth = baseWidth\n        self.scale = scale\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, 3, 1, 1, bias=False)\n        )\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(3, 2, 1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.AvgPool2d(stride, stride, ceil_mode=True, count_include_pad=False),\n                nn.Conv2d(self.inplanes, planes * block.expansion, 1, 1, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, stype='stage', baseWidth=self.baseWidth, scale=self.scale))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, baseWidth=self.baseWidth, scale=self.scale))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x_start_pool = self.maxpool(x)\n        x1 = self.layer1(x_start_pool)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n        return x1, x2, x3, x4\n\ndef res2net50_v1b_26w_4s(pretrained=False, **kwargs):\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth=26, scale=4, **kwargs)\n    if pretrained:\n        print(\"Loading Res2Net50 weights...\")\n        state_dict = model_zoo.load_url(model_urls['res2net50_v1b_26w_4s'], map_location='cpu')\n        model.load_state_dict(state_dict, strict=False)\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/lib/Network_Res2Net_GRA_NCD.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom camoXpert.lib.Res2Net_v1b import res2net50_v1b_26w_4s\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation, bias=False)\n        self.bn = nn.BatchNorm2d(out_planes)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n\nclass RFB_modified(nn.Module):\n    def __init__(self, in_channel, out_channel): super(RFB_modified, self).__init__()\n    def forward(self, x): return x\n\nclass NeighborConnectionDecoder(nn.Module):\n    def __init__(self, channel): super(NeighborConnectionDecoder, self).__init__()\n    def forward(self, x1, x2, x3): pass\n\nclass Network(nn.Module):\n    def __init__(self, channel=32, imagenet_pretrained=True):\n        super(Network, self).__init__()\n        self.resnet = res2net50_v1b_26w_4s(pretrained=imagenet_pretrained)\n    def forward(self, x):\n        x_start = self.resnet.conv1(x)\n        x_start = self.resnet.bn1(x_start)\n        x_start = self.resnet.relu(x_start)\n        x_start_pool = self.resnet.maxpool(x_start) \n        x1 = self.resnet.layer1(x_start_pool)\n        x2 = self.resnet.layer2(x1)\n        x3 = self.resnet.layer3(x2)\n        x4 = self.resnet.layer4(x3)\n        return x1, x2, x3, x4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/models/sinet_encoder.py\nimport torch\nimport torch.nn as nn\nimport sys\nimport os\nfrom camoXpert.lib.Network_Res2Net_GRA_NCD import Network as SINet_Network\n\nclass SINetEncoder(nn.Module):\n    def __init__(self, pretrained=True):\n        super().__init__()\n        self.backbone = SINet_Network(imagenet_pretrained=pretrained)\n        \n        self.feature_info = [\n            {'num_chs': 256, 'reduction': 4, 'module': 'layer1'},\n            {'num_chs': 512, 'reduction': 8, 'module': 'layer2'},\n            {'num_chs': 1024, 'reduction': 16, 'module': 'layer3'},\n            {'num_chs': 2048, 'reduction': 32, 'module': 'layer4'}\n        ]\n\n    def forward(self, x):\n        return self.backbone(x)\n\n    def channels(self):\n        return [info['num_chs'] for info in self.feature_info]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 11: Run the training","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/camoXpert/scripts/main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:48:19.655916Z","iopub.execute_input":"2025-11-13T08:48:19.656391Z","iopub.status.idle":"2025-11-13T08:49:55.912718Z","shell.execute_reply.started":"2025-11-13T08:48:19.656371Z","shell.execute_reply":"2025-11-13T08:49:55.911876Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\nRunning with 'faked' args: ['train', '--dataset-path', '/kaggle/input/cod10k-dataset/COD10K-v3', '--checkpoint-dir', '/kaggle/working/camoXpert/results/resnet_checkpoints', '--resume-from', '/kaggle/working/camoXpert/results/resnet_checkpoints/best_model.pth', '--backbone', 'res2net_ncd', '--num-experts', '3', '--batch-size', '8', '--img-size', '352', '--epochs', '50', '--deep-supervision']\n--- Using Device: cuda ---\n--- Loading Data from /kaggle/input/cod10k-dataset/COD10K-v3 ---\nLoading dataset: COD10K (train)\nFound 6000 images in /kaggle/input/cod10k-dataset/COD10K-v3/Train/Image\nLoading dataset: COD10K (test)\nFound 4000 images in /kaggle/input/cod10k-dataset/COD10K-v3/Test/Image\n--- Loading Model ---\n\n======================================================================\nINITIALIZING CAMOXPERT (OPTIMIZED HYBRID - NO SDTA)\n======================================================================\nBackbone: res2net_ncd, Pretrained: True, Experts: 3\nLoading Res2Net50 weights...\nEncoder Dims: [256, 512, 1024, 2048]\nUsing 2 GPUs for TRAINING! (DataParallel)\nTotal Params: 178.00M | Trainable: 178.00M\nResuming training from checkpoint: /kaggle/working/camoXpert/results/resnet_checkpoints/best_model.pth\nLoaded scheduler state.\nResumed. Starting from Epoch 30, Best IoU: 0.6842\n--- Starting Training from Epoch 31 to 50 ---\n\n--- Epoch 31/50 ---\nTraining:   0%|                                                             | 0/750 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nTraining:  19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                         | 142/750 [01:25<06:00,  1.69it/s]^C\nTraining:  19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                                         | 142/750 [01:26<06:09,  1.65it/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/camoXpert/scripts/main.py\", line 239, in <module>\n    main()\n  File \"/kaggle/working/camoXpert/scripts/main.py\", line 236, in main\n    run_training(args)\n  File \"/kaggle/working/camoXpert/scripts/main.py\", line 165, in run_training\n    train_loss = train_one_epoch(\n                 ^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/camoXpert/scripts/main.py\", line 44, in train_one_epoch\n    scaler.step(optimizer)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\", line 457, in step\n    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\", line 351, in _maybe_opt_step\n    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\", line 351, in <genexpr>\n    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n               ^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Block 12: Download the trained model","metadata":{}},{"cell_type":"code","source":"!cp /kaggle/working/camoXpert/results/resnet_checkpoints/best_model.pth /kaggle/working/best_model.pth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm /kaggle/working/best_model_DOWNLOAD.pth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 13: Download test datasets","metadata":{}},{"cell_type":"code","source":"# Install gdown to download from Google Drive\n!pip install -q gdown\n\n# Download the test datasets zip file\n# This ID is from the README.md link: https://drive.google.com/file/d/1V0iSEdYJrT0Y_DHZfVGMg6TySFRNTy4o/view?usp=sharing\n!gdown 1V0iSEdYJrT0Y_DHZfVGMg6TySFRNTy4o -O /kaggle/working/TestDataset.zip\n\n# Unzip the datasets\n!unzip -q /kaggle/working/TestDataset.zip -d /kaggle/working/TestDataset/\n!echo \"Test datasets downloaded and unzipped.\"\n!ls /kaggle/working/TestDataset/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working/TestDataset/COD-TestDataset/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 14: Write Testing script","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/camoXpert/scripts/test.py\n\"\"\"\nCamoXpert - Evaluation Script\n(Corrected Version - Finds 'Imgs' and 'GT' folders)\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport yaml\nimport cv2\nimport numpy as np\n\n# --- Add project root to Python path ---\nPROJECT_ROOT = '/kaggle/working' \nsys.path.insert(0, PROJECT_ROOT)\n\n# --- Imports from our project ---\nfrom camoXpert.models.camoxpert import CamoXpert\nfrom camoXpert.models.utils import count_parameters, set_seed, load_config, AverageMeter\nfrom camoXpert.metrics.cod_metrics import CODMetrics\nfrom camoXpert.data.augmentations import AdvancedCODAugmentation\n\n# --- Special Dataset Class for Testing (Smarter Version) ---\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, root_dir, img_size=352):\n        self.img_size = img_size\n        \n        # --- Find Image Directory (Checks for 'Image' or 'Imgs') ---\n        if os.path.exists(os.path.join(root_dir, 'Image')):\n            self.image_dir = os.path.join(root_dir, 'Image')\n        elif os.path.exists(os.path.join(root_dir, 'Imgs')):\n            self.image_dir = os.path.join(root_dir, 'Imgs')\n        else:\n            raise FileNotFoundError(f\"Could not find 'Image' or 'Imgs' folder in {root_dir}\")\n            \n        # --- Find Mask Directory (Checks for 'GT' or 'Mask') ---\n        if os.path.exists(os.path.join(root_dir, 'GT')):\n            self.mask_dir = os.path.join(root_dir, 'GT')\n        elif os.path.exists(os.path.join(root_dir, 'Mask')):\n            self.mask_dir = os.path.join(root_dir, 'Mask')\n        else:\n            raise FileNotFoundError(f\"Could not find 'GT' or 'Mask' folder in {root_dir}\")\n\n        self.image_list = sorted([f for f in os.listdir(self.image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n        \n        self.transform = AdvancedCODAugmentation(img_size=img_size, is_train=False)\n        print(f\"Loaded {len(self.image_list)} images from {self.image_dir}\")\n\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, idx):\n        img_name = self.image_list[idx]\n        img_path = os.path.join(self.image_dir, img_name)\n        \n        # Try .png first, then .jpg for mask\n        mask_name_png = os.path.splitext(img_name)[0] + '.png'\n        mask_path = os.path.join(self.mask_dir, mask_name_png)\n        \n        if not os.path.exists(mask_path):\n            mask_name_jpg = os.path.splitext(img_name)[0] + '.jpg'\n            mask_path = os.path.join(self.mask_dir, mask_name_jpg)\n            \n            if not os.path.exists(mask_path):\n                 # Fallback for NC4K which might have matching names\n                 mask_path = os.path.join(self.mask_dir, img_name)\n\n        image = cv2.imread(img_path)\n        if image is None:\n             raise IOError(f\"Could not read image: {img_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        if mask is None:\n            raise IOError(f\"Could not read mask for image: {img_name} at {mask_path}\")\n\n        mask = (mask > 128).astype(np.float32)\n        transformed = self.transform(image=image, mask=mask)\n        \n        return transformed['image'], transformed['mask'].unsqueeze(0)\n\n# --- Validation Function (Copied from main.py) ---\n@torch.no_grad()\ndef validate(model, dataloader, metrics, device):\n    model.eval()\n    all_metrics = []\n    \n    for images, masks in tqdm(dataloader, desc=\"Validating\", ncols=100):\n        images, masks = images.to(device), masks.to(device)\n        pred, _, _ = model(images, return_deep_supervision=False)\n        batch_metrics = metrics.compute_all(pred, masks)\n        all_metrics.append(batch_metrics)\n        \n    agg_metrics = {key: (sum(d[key] for d in all_metrics) / len(all_metrics)) for key in all_metrics[0]}\n    return agg_metrics\n\n# --- Main Test Function ---\ndef run_test(args):\n    set_seed(42)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"--- Using Device: {device} ---\")\n\n    print(\"--- Loading Model ---\")\n    model = CamoXpert(\n        num_classes=1,\n        backbone=args.backbone,\n        num_experts=3,\n        pretrained=False # We are loading our own weights, not ImageNet\n    ).to(device)\n\n    # --- Load Your Trained Weights ---\n    print(f\"Loading trained weights from: {args.model_path}\")\n    checkpoint = torch.load(args.model_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    total, _ = count_parameters(model)\n    print(f\"Total Params: {total/1.e6:.2f}M\")\n    \n    metrics = CODMetrics(device=device)\n\n    # --- Test on all datasets ---\n    test_datasets = ['CHAMELEON', 'CAMO', 'NC4K', 'COD10K']\n    results = {}\n\n    for dataset_name in test_datasets:\n        print(f\"\\n{'='*70}\\n--- TESTING ON: {dataset_name} ---\\n{'='*70}\")\n        dataset_path = os.path.join(args.dataset_dir, dataset_name)\n        \n        if not os.path.exists(dataset_path):\n            print(f\"Warning: Dataset not found at {dataset_path}. Skipping.\")\n            continue\n            \n        try:\n            test_dataset = TestDataset(root_dir=dataset_path, img_size=args.img_size)\n        except FileNotFoundError as e:\n            print(f\"Error loading {dataset_name}: {e}. Skipping.\")\n            continue\n            \n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=args.batch_size, # Use a large batch for fast validation\n            shuffle=False,\n            num_workers=2,\n            pin_memory=True\n        )\n        \n        val_metrics = validate(model, test_loader, metrics, device)\n        results[dataset_name] = val_metrics\n        print(f\"--- Results for {dataset_name} ---\")\n        print(f\"  IoU:  {val_metrics['IoU']:.4f}\")\n        print(f\"  Dice: {val_metrics['Dice_Score']:.4f}\")\n\n    print(f\"\\n\\n{'='*70}\\n--- FINAL SUMMARY ---\\n{'='*70}\")\n    for dataset_name, res in results.items():\n        print(f\"  {dataset_name}: IoU = {res['IoU']:.4f}, Dice = {res['Dice_Score']:.4f}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description='CamoXpert Evaluation')\n    parser.add_argument('--model-path', type=str, required=True, help=\"Path to your trained best_model.pth\")\n    # --- PATH UPDATED ---\n    parser.add_argument('--dataset-dir', type=str, default='/kaggle/working/TestDataset/COD-TestDataset', help=\"Directory containing test datasets\")\n    parser.add_argument('--backbone', type=str, default='edgenext_small', help=\"Backbone of the loaded model\")\n    parser.add_argument('--img-size', type=int, default=352)\n    parser.add_argument('--batch-size', type=int, default=16) # Batch size for validation\n    \n    args = parser.parse_args()\n    run_test(args)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working/TestDataset/COD-TestDataset/CHAMELEON/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 15: Run the test","metadata":{}},{"cell_type":"code","source":"!python /kaggle/working/camoXpert/scripts/test.py \\\n    --model-path \"/kaggle/working/camoXpert/results/resnet_checkpoints/best_model.pth\" \\\n    --backbone \"res2net_ncd\" \\\n    --batch-size 8","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Block 16: Visualize results","metadata":{}},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport sys\n\n# --- Add project root to Python path ---\nPROJECT_ROOT = '/kaggle/working' \nsys.path.insert(0, PROJECT_ROOT)\n\nfrom camoXpert.models.camoxpert import CamoXpert\nfrom camoXpert.scripts.test import TestDataset # Re-use our smart TestDataset class\n\n# --- Configuration (UPDATED FOR RESNET) ---\nMODEL_PATH = \"/kaggle/working/camoXpert/results/resnet_checkpoints/best_model.pth\"\nBACKBONE_NAME = \"res2net_ncd\" # <-- UPDATED\nDATASET_PATH = \"/kaggle/working/TestDataset/COD-TestDataset/CHAMELEON\"\nIMG_SIZE = 352\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- Load Model ---\nprint(\"Loading RESNET model for visualization...\")\nmodel = CamoXpert(backbone=BACKBONE_NAME, pretrained=False).to(DEVICE)\ncheckpoint = torch.load(MODEL_PATH, map_location=DEVICE)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\nprint(\"Model loaded.\")\n\n# --- Load Dataset ---\ndataset = TestDataset(root_dir=DATASET_PATH, img_size=IMG_SIZE)\n# Get 5 random images\nrandom_indices = random.sample(range(len(dataset)), 5)\n\n# --- Create Plot ---\nfig, axes = plt.subplots(5, 3, figsize=(15, 25))\nfig.suptitle(f\"Model Qualitative Results (Backbone: {BACKBONE_NAME})\", fontsize=20, y=1.02)\naxes[0, 0].set_title(\"Original Image\", fontsize=16)\naxes[0, 1].set_title(\"Ground Truth\", fontsize=16)\naxes[0, 2].set_title(\"Prediction\", fontsize=16)\n\nwith torch.no_grad():\n    for i, idx in enumerate(random_indices):\n        # 1. Get data\n        image_tensor, mask_tensor = dataset[idx]\n        image_tensor = image_tensor.to(DEVICE).unsqueeze(0) # Add batch dim\n\n        # 2. Run prediction\n        pred_tensor, _, _ = model(image_tensor, return_deep_supervision=False)\n        pred_mask = torch.sigmoid(pred_tensor).cpu().numpy().squeeze()\n        pred_mask = (pred_mask > 0.5).astype(np.uint8)\n\n        # 3. Get original image for plotting (un-normalize)\n        img_name = dataset.image_list[idx]\n        original_image = cv2.imread(os.path.join(dataset.image_dir, img_name))\n        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n        original_image = cv2.resize(original_image, (IMG_SIZE, IMG_SIZE))\n        \n        # 4. Get ground truth mask\n        ground_truth_mask = mask_tensor.cpu().numpy().squeeze()\n\n        # 5. Plot\n        axes[i, 0].imshow(original_image)\n        axes[i, 0].axis('off')\n        axes[i, 1].imshow(ground_truth_mask, cmap='gray')\n        axes[i, 1].axis('off')\n        axes[i, 2].imshow(pred_mask, cmap='gray')\n        axes[i, 2].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}